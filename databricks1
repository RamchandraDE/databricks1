# Databricks notebook source
dbutils.fs.mount(
    source= 'wasbs://container@sgnumber1.blob.core.windows.net',
    mount_point="/mnt/df",
    extra_configs={
        'fs.azure.account.key.sgnumber1.blob.core.windows.net': 'Qt/ppFsykYjIETj8xEOND0NG3FMydAlMMCWnP8Yd2nmlHuWDwGoQKeSFW2ypeDiLrLWKItEp2WPZ+ASt8QzRHw=='
    }
)

# COMMAND ----------

dbutils.fs.ls("/mnt/df/Raw/")

# COMMAND ----------

# Replace 'your_csv_file.csv' with the actual path to your CSV file
df_csv = spark.read.csv('dbfs:"/mnt/df/Raw/", header=True, inferSchema=True)



# COMMAND ----------

employeeattendance = [spark.read.csv(file_path, header=True, inferSchema=True)"/mnt/df/container/raw/employeeattendance.csv"]

# COMMAND ----------

from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("ReadCSVFiles").getOrCreate()

# List of file paths
file_paths = [
    'dbfs:/mnt/df/Raw/employeeattendance.csv',
    'dbfs:/mnt/df/Raw/employeebenefits.csv',
    'dbfs:/mnt/df/Raw/employeereviews.csv',
    'dbfs:/mnt/df/Raw/employees.csv',
    'dbfs:/mnt/df/Raw/employeetraining.csv',
    'dbfs:/mnt/df/Raw/shifts.csv',
    'dbfs:/mnt/df/Raw/stations.csv'
]

# Read each CSV file into a DataFrame
dataframes = [spark.read.csv(file_path, header=True, inferSchema=True) for file_path in file_paths]

# Display the contents of each DataFrame
for i, df in enumerate(dataframes):
    print(f"DataFrame {i + 1}: {file_paths[i]}")
    df.show(truncate=False)


# COMMAND ----------

# Assuming you have an 'employeeattendance' DataFrame
employeeattendance = spark.read.csv('dbfs:/mnt/df/Raw/employeeattendance.csv', header=True, inferSchema=True)

# Display the original DataFrame
print("Original DataFrame:")
employeeattendance.show(truncate=False)

# Delete rows where 'attendance_status' is equal to 'Absent'
employeeattendance = employeeattendance.filter(employeeattendance['attendance_ID'] = 1,3)

# Display the modified DataFrame
print("DataFrame after deleting rows:")
employeeattendance.show(truncate=False)



# COMMAND ----------

employeereviews = spark.read.csv('dbfs:/mnt/df/Raw/employeereviews.csv', header=True, inferSchema=True)
employeebenefits = spark.read.csv('dbfs:/mnt/df/Raw/employeebenefits.csv', header=True, inferSchema=True)

# COMMAND ----------

display(employeereviews)
display(employeebenefits)

# COMMAND ----------

employeeattendance = employeeattendance.filter(employeeattendance['AttendanceID'] != 1)

# COMMAND ----------

display(employeereviews)
display(employeebenefits)

# COMMAND ----------

full_join_result = employeebenefits.join(employeereviews, "EmployeeID", "left")

# COMMAND ----------

display(full_join_result)

# COMMAND ----------

full_join_result = employeereviews.join(employeebenefits, "EmployeeID", "right")

# COMMAND ----------

display(full_join_result)

# COMMAND ----------

full_join_result = employeereviews.join(employeebenefits, "EmployeeID", "inner")

# COMMAND ----------

display(full_join_result)

# COMMAND ----------

dbutils.fs.ls('/mnt/df/Silver')

# COMMAND ----------

dbutils.fs.mounts()

# COMMAND ----------

full_join_result.repartition(1).write.mode("overwrite").parquet("dbfs:/mnt/df/Silver/full_join")

# COMMAND ----------

full_join_result.repartition(1).write.mode("overwrite").parquet("/mnt/adlsgen2project1/silver/innerjo/")